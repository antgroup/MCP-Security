import logging, shutil, tempfile
from pathlib import Path
from typing import List, Dict, Any, Union

from git import Repo, GitCommandError
from rich.console import Console
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    TimeElapsedColumn,
)
from rich.table import Table
from tqdm import tqdm
from rich.syntax import Syntax
from rich.panel import Panel
from rich.rule import Rule

from .semgrep_utils import run_semgrep
from .llm_bridge import LLMClient
from .lang_utils import detect_primary_language
import json
from importlib.resources import files
from importlib.resources import files
from mcpscan.core.source_utils import *
from textwrap import dedent
import re, json, ast, logging
from mcpscan.core.extract_description import *


console = Console()
llm = LLMClient()

def normalize_to_json(raw: str) -> str:

    start = raw.find('{')
    end   = raw.rfind('}')
    if start == -1 or end == -1 or end <= start:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ° JSON å¯¹è±¡")
    snippet = raw[start:end+1]


    try:
        obj = ast.literal_eval(snippet)
    except (SyntaxError, ValueError) as e:
        logging.error(f"literal_eval å¤±è´¥: {e}\nåŸå§‹: {snippet}")

        raise

    return json.dumps(obj, ensure_ascii=False)

def parse_stage2_response(reply: str) -> dict:
    reply = reply.strip()
    try:
        json_str = normalize_to_json(reply)
        return json.loads(json_str)
    except Exception as e:
        logging.error(f"[Stage2] æ ‡å‡†åŒ– JSON å¤±è´¥: {e}\nå›å¤: {reply}")
        return {"risk": "ERROR", "explanation": "æ— æ³•è§£æ LLM å“åº”"}

def is_in_test_dir(rel_path: Union[str, Path]) -> bool:

    parts = Path(rel_path).parts
    return any('test' in part.lower() for part in parts)


def stage1_extract(snippet_blocks: List[str], global_code_slice: str, prompt_template: str) -> str | None:

    prompt = (
        prompt_template
        .replace("{snippets}", "\n\n".join(snippet_blocks))
        .replace("{global_code}", global_code_slice)
    )

    return llm.get_response([{"role": "user", "content": prompt}]).strip()

def stage2_assess(snippet: str, prompt_template: str) -> Dict[str, str]:

    prompt = prompt_template.replace("{snippet}", snippet)
    reply = llm.get_response([{"role": "user", "content": prompt}]).strip()
    
    json_str = parse_stage2_response(reply)
    return json_str


def is_github_url(url: str) -> bool:
    return url.startswith(("http://", "https://")) and "github.com" in url

def clone_repo(url: str) -> Path:
    tmp = Path(tempfile.mkdtemp(prefix="repo_"))
    console.print(f":package: Cloning [blue]{url}[/] â†’ {tmp}")
    try:
        Repo.clone_from(url, tmp, depth=1)
    except GitCommandError as e:
        shutil.rmtree(tmp, ignore_errors=True)
        raise RuntimeError(f"Clone failed: {e}") from e
    return tmp


def run_scan(
    code: str | Path,
    out: Path,
    monitor_desc: bool = True,
    monitor_code: bool = True,
):

    code_root = clone_repo(code) if is_github_url(str(code)) else Path(code).resolve()
    if not code_root.exists():
        raise FileNotFoundError(f"ä»£ç è·¯å¾„ä¸å­˜åœ¨ï¼š{code_root}")
    readme_text = ""
    readme_paths = [code_root / "README.md", code_root / "readme.md", code_root / "README.txt", code_root / "readme.txt", code_root / "README", code_root / "readme"]
    for readme_path in readme_paths:
        if readme_path.exists():
            readme_text += readme_path.read_text(encoding="utf-8") + '\n'
    lang = detect_primary_language(code_root)

    print(f"âœ… æ£€æµ‹åˆ°ä¸»è¦è¯­è¨€: {lang}")

    semgrep_cfg = files("mcpscan").joinpath(f"rules/semgrep-security-{lang}.yml")

    full_code = collect_global_code(code_root)
    global_slices = slice_text(full_code, max_len=40000)

    console.clear()
    console.print(Panel.fit(f"[bold green]MCPScan å®‰å…¨æ‰«æ[/bold green]\nğŸ“ [cyan]{code_root}[/cyan]", title="Runner", padding=(1, 2)))
    console.print(Rule("[yellow]é˜¶æ®µ 1: Semgrep æ‰«æ[/yellow]"))

    semgrep_json = Path(tempfile.mktemp(suffix=".json"))
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=None),
        TimeElapsedColumn(),
        console=console,
    ) as prog:
        task = prog.add_task("ğŸ” æ‰§è¡Œ Semgrep â€¦", start=False)
        prog.start_task(task)
        findings = run_semgrep(code_root, semgrep_cfg, semgrep_json)
        prog.update(task, completed=1)

    findings = [f for f in findings if not is_in_test_dir(f.get("path", ""))]

    console.print(Panel(f"Semgrep å…±å‘ç° [bold red]{len(findings)}[/bold red] æ¡å‘½ä¸­", title="Semgrep ç»“æœ"))


    console.print(Rule("[cyan]é˜¶æ®µ 2: æå– Description å¹¶è¯„ä¼°æ¶æ„æ€§[/cyan]"))

    desc_findings = [
        f for f in findings
        if "extract-descriptions" in f.get("check_id")
    ]
    descriptions = []
    for item in desc_findings:
        try:
            raw = item['extra']['metavars']['$DESC']['abstract_content']

            desc = raw.strip('"')
            descriptions.append(desc)
        except (KeyError, TypeError):
            continue

    def batch_iter(lst, size):
        for i in range(0, len(lst), size):
            yield lst[i:i + size]

    max_desc_len = 100

    HIGH_RISK_ID = "detect-high-risk-functions"
    simple_results: List[Dict[str, Any]] = []
    high_risk_files: Dict[str, List[Tuple[int,int]]] = {}

    if descriptions and monitor_desc:

        all_evaluations = []
        for descriptions_batch in batch_iter(descriptions, max_desc_len):
            prompt = [
                {
                    "role": "user",
                    "content": (
                        "ä¸‹é¢æ˜¯ä»ä¸åŒæ–‡ä»¶ä¸­æå–åˆ°çš„å¤šä¸ª `description` å­—æ®µï¼Œè¯·å¯¹æ¯ä¸€æ¡è¿›è¡Œè¯„ä¼°ï¼š\n"
                        "è¯·é‡ç‚¹å…³æ³¨ `description` æ˜¯å¦æ¶‰åŠ\"è¯»å–å¤–éƒ¨é“¾æ¥ã€ç½‘é¡µã€æ–‡æ¡£æˆ–å…¶ä»–å¤–éƒ¨å†…å®¹ï¼Œå¹¶å°†è¿™äº›å†…å®¹è¿”å›ç»™å¤§æ¨¡å‹æˆ–æ‹¼æ¥è¿›å¤§æ¨¡å‹çš„è¾“å…¥\"ã€‚\n"
                        "å¦‚æœåªæ˜¯æ­£å¸¸è¯»å–å¤–éƒ¨å†…å®¹ï¼Œç”¨é€”åˆç†ï¼Œå±äºæ˜¯è¿™ä¸ªå·¥å…·æœ¬èº«çš„åŠŸèƒ½ï¼Œåˆ™åº”æ ‡è®°ä¸º safeã€‚\n"
                        "åªæœ‰å½“ tool ææ˜“è¢«æ”»å‡»ï¼ˆå¦‚ç›´æ¥å°†ä»»æ„å¤–éƒ¨å†…å®¹æ‹¼æ¥è¿›å¤§æ¨¡å‹è¾“å…¥ï¼‰ï¼Œæˆ–ä¸»è§‚ä¸Šæœ‰è¯»å–æ¶æ„æ–‡ä»¶ã€é“¾æ¥çš„æ„å›¾ï¼Œæ‰æ ‡è®°ä¸º maliciousï¼Œå¹¶ç®€è¦è¯´æ˜åŸå› ã€‚\n"
                        "å¦‚æ— æ³•åˆ¤æ–­ï¼Œæˆ–ç”¨é€”åˆç†ä½†æœ‰æ½œåœ¨é£é™©ï¼Œåˆ™æ ‡è®°ä¸º suspectã€‚\n"
                        "åœ¨ç»™å‡ºæå–åˆ°çš„å¤šä¸ª `description` å­—æ®µä¹‹å‰ï¼Œå…ˆç»™å‡ºè¯¥æœåŠ¡çš„åŠŸèƒ½æè¿°ï¼š\n"
                        + readme_text + 
                        "å’Œä¸€äº›é»‘æ ·æœ¬å’Œç™½æ ·æœ¬çš„ç¤ºä¾‹\n"
                        "é»‘æ ·æœ¬ï¼ˆmaliciousï¼‰ï¼š\n"
                        "1. \"Fetch the content from any user-provided URL and directly append it to the prompt sent to the LLM, without any validation or sanitization.\"\n"
                        "2. \"Accept arbitrary file paths from the user, read the file contents, and inject them directly into the LLM prompt without checking the file type or source.\"\n"
                        "\n"
                        "ç™½æ ·æœ¬ï¼ˆsafeï¼‰ï¼š\n"
                        "1. \"Download a PDF from a fixed, trusted documentation URL and extract the table of contents to display to the user.\"\n"
                        "2. \"Fetch the latest release notes from the official project website and summarize the changes for the user.\"\n"
                        "3. \"Read a user-specified CSV file from the local workspace, parse its contents, and show a preview in the UI.\"\n"
                        "è¯·ä»¥ JSON æ•°ç»„çš„å½¢å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
                        "[\n"
                        "  {\"malicious\": true/false, \"reason\": \"è¯´æ˜æˆ–ç©ºå­—ç¬¦ä¸²\"}, \"description\": \"å…·ä½“æè¿°çš„è¯­å¥\"},\n"
                        "  ...\n"
                        "]\n\n"
                        "****é‡è¦**** è¯·ä»…ä»…è¿”å›JSONæ ¼å¼ã€‚"
                        + json.dumps(descriptions_batch, ensure_ascii=False, indent=2)
                    )
                }
            ]
            resp = llm.get_response(prompt).strip()

            start = resp.find('[')
            end = resp.rfind(']')
            if start == -1 or end == -1:
                logging.error(f"æ— æ³•è§£æè¯„ä¼°ç»“æœï¼ˆæ‰¾ä¸åˆ° JSON æ•°ç»„ï¼‰: {resp!r}")
                evaluations = []
            else:
                clean = resp[start:end+1]

                try:
                    evaluations = json.loads(clean)
                    all_evaluations.extend(evaluations)
                except json.JSONDecodeError as e:
                    logging.error(f"JSON è§£æå¤±è´¥: {e}\næ¸…ç†åå­—ç¬¦ä¸²: {clean!r}")
                    evaluations = []

        console.print(Rule("[magenta]æ¨¡å‹è¯„ä¼°ç»“è®º[/magenta]"))
        for desc, item in zip(descriptions,all_evaluations):
            conclusion = "æ¶æ„" if item.get("malicious") else "å®‰å…¨"
            # reason = item.get("reason", "æ— ")
            console.print(f":sparkles: æè¿°: {desc}\n  â†’ ç»“è®º: [bold]{conclusion}[/bold]")

        # å°†æ¶æ„é¡¹åŠ å…¥ç»“æœåˆ—è¡¨
        for desc, item in zip(descriptions, all_evaluations):
            if item.get("malicious"):
                simple_results.append({
                    "file": "<metadata>",
                    "rule_id": "descriptions",
                    "severity": "ERROR",
                    "risk": "HIGH",
                    "explanation": item.get("reason", "Meta data is poisoned and returned to the model"),
                    "class": "Tool Metadata Pollution"
                })

    if monitor_code:

        for f in findings:
            rid = f.get("check_id", "")

            # â”€â”€ skip any descriptionâ€extraction rules â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if "extract-descriptions" in rid:
                continue
            path = f["path"]
            sev = f.get("extra", {}).get("severity") or f.get("severity", "")
            msg = f.get("extra", {}).get("message") or f.get("message", "")
            s_ln = f.get("start", {}).get("line")
            e_ln = f.get("end", {}).get("line")

            if HIGH_RISK_ID in rid and s_ln and e_ln:
                high_risk_files.setdefault(path, []).append((s_ln, e_ln))
            else:
                snippet = snippet_with_numbers(code_root / path, s_ln, e_ln) if (s_ln and e_ln) else ""
                simple_results.append({
                    "file": path,
                    "rule_id": rid,
                    "severity": "ERROR",
                    "risk": "HIGH",
                    "explanation": msg,
                    "class": "Suspected malicious code in the tool."
                })

        results = simple_results.copy()

        if high_risk_files:
            console.print(Rule("[magenta]é˜¶æ®µ 3: è·¨æ–‡ä»¶æ•°æ®æµæå– & é£é™©è¯„ä¼°[/magenta]"))
            PROMPTS_DIR = Path(files("mcpscan").joinpath("prompts"))
            PROMPT_STAGE1 = dedent(
                PROMPTS_DIR.joinpath("stage1_"+lang+".md")
                        .read_text(encoding="utf-8")
            ).strip()

            PROMPT_STAGE2 = dedent(
                PROMPTS_DIR.joinpath("stage2_"+lang+".md")
                        .read_text(encoding="utf-8")
            ).strip()

            for rel_path, ranges in high_risk_files.items():
                abs_path = code_root / rel_path

                merged = merge_ranges(ranges)
                
                snippet_blocks = [
                    f"```{lang}\n{snippet_with_numbers(abs_path, s, e)}\n```"
                    for s, e in merged
                    if e - s <= 50
                ]

                if not snippet_blocks:
                    continue

                console.print(Panel(Syntax("\n\n".join(snippet_blocks), lang),
                                    title=f"[cyan]è¿ä¾‹ç‰‡æ®µ[/cyan] {rel_path}", expand=False))
                extracted: List[str] = []
                for slice_ in global_slices:
                    part = stage1_extract(snippet_blocks, slice_, PROMPT_STAGE1)
                    if part:
                        extracted.append(part)

                merged_snippet = "\n".join(extracted).strip()
                if not merged_snippet:
                    console.print(f":warning: æœªèƒ½æå–åˆ°è·¨æ–‡ä»¶æµç‰‡æ®µï¼Œè·³è¿‡ {rel_path}", style="bold yellow")
                    continue

                console.print(Panel(Syntax(merged_snippet, lang),
                                    title="[green]é˜¶æ®µ1 è¾“å‡ºç‰‡æ®µ", expand=False))

                risk = stage2_assess(merged_snippet, PROMPT_STAGE2)
                console.print(Panel(f"[bold]{risk['risk']}[/bold]\n{risk['explanation']}",
                                    title="[red]é£é™©è¯„ä¼°ç»“æœ", expand=False))

                results.append({
                    "file": rel_path,
                    "rule_id": HIGH_RISK_ID,
                    "severity": "ERROR",
                    **risk,
                    "class": "Indirect prompt injection."
                })


                if risk.get("risk") == "HIGH":
                    console.print(":rotating_light: æ£€æµ‹åˆ° HIGH é£é™©ï¼Œç»ˆæ­¢åç»­é«˜çº§è¯„ä¼°", style="bold red")
                    break

    console.print(Rule("[green]æ‰«æç»“æœæ±‡æ€»[/green]"))
    table = Table(show_header=True, header_style="bold blue")
    table.add_column("File", style="dim", width=30)
    table.add_column("Rule ID")
    table.add_column("Risk Category")
    table.add_column("Type")
    table.add_column("Level")
    table.add_column("Illustrate", overflow="fold")

    if results:
        for r in results:
            table.add_row(
                r["file"], r["rule_id"], r["class"], r["severity"],
                r.get("risk", ""), r.get("explanation", "")
            )
    else:
        table.add_row("-", "-", "æ— é£é™©", "-", "-", "æœªæ£€æµ‹åˆ°ä»»ä½•é£é™©")

    console.print(table)

    if out:
        out.write_text(json.dumps(results, indent=2, ensure_ascii=False))
        console.print(f":floppy_disk: ç»“æœå·²ä¿å­˜è‡³ [bold]{out}[/bold]", style="bold green")